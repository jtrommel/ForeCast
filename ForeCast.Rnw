\documentclass[a4paper,11pt]{article}
\usepackage{graphicx} % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[total={424pt,600pt},top=100pt,left=90pt]{geometry} % instelling van de paginaindeling
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage{layout} % Gebruik in het begin om de layout-elementen van het document te verifiÃ«ren
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\title{Forecasting methods}
\author{Jan Trommelmans}

\begin{document}
\date{}
\SweaveOpts{concordance=TRUE,prefix.string=ForeCast}
\maketitle

<<>>=
library(tidyverse)
library(lubridate)
library(gridExtra)
library(forecast)
@

\section{Forecasting methods}

\url{https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/}

\subsection{Start series}
<<label=basis,fig=TRUE,include=FALSE, echo=TRUE>>=
x <- c(1:7)
series.1 <- c(3, 10, 12, 13, 12, 10, 12)
serie <- data.frame(x=x,y=series.1)
ggplot(data=serie) + geom_point(aes(x=x, y=y))
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-basis}
\captionof{figure}{start series}
\label{fig:basis}
\end{center}

\subsection{Naive method}

$ \hat{y}_{i} = y_{i-1} $

<<label=naive,fig=TRUE,include=FALSE, echo=TRUE>>=
serie.naive <- serie
serie.naive[nrow(serie.naive)+1,] <- c(nrow(serie.naive)+1,0)
serie.naive$yhat <- NA
for (i in (2:nrow(serie.naive))) {
  serie.naive$yhat[i] <- serie.naive$y[i-1]
}
ggplot(data=serie.naive,aes(x=x)) + geom_point(aes(y=y),colour="black") + geom_point(aes(y=yhat),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-naive}
\captionof{figure}{Naive}
\label{fig:naive}
\end{center}

\subsection{Simple average}

$ \hat{y}_{i} = \frac{1}{i-1}\sum_{j=1}^{j=i-1}y_{j} $

<<label=sa,fig=TRUE,include=FALSE, echo=TRUE>>=
serie.sa <- serie
serie.sa[nrow(serie.sa)+1,] <- c(nrow(serie.sa)+1,NA)
serie.sa$cumsum <- cumsum(serie.sa$y)
for (i in (2:nrow(serie.sa))) {
  serie.sa$yhat[i] <- serie.sa$cumsum[i-1]/serie.sa$x[i-1]
}
ggplot(data=serie.sa,aes(x=x)) + geom_point(aes(y=y),colour="black") + geom_point(aes(y=yhat),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-sa}
\captionof{figure}{Simple average}
\label{fig:sa}
\end{center}

\subsection{Moving average with size n}

n=3

$ \hat{y}_{i} = \frac{1}{n}\sum_{j=i-n}^{j=i-1}y_{j} $

<<label=ma,fig=TRUE,include=FALSE, echo=TRUE>>=
n <- 3
serie.ma <- serie
serie.ma[nrow(serie.ma)+1,] <- c(nrow(serie.ma)+1,NA)
for (i in ((n+1):nrow(serie.ma))) {
  som <- 0
  for (j in (1:n)) {
    som <- som + serie.ma$y[i-j]
  } 
  serie.ma$yhat[i] <- som/n
}
ggplot(data=serie.ma,aes(x=x)) + geom_point(aes(y=y),colour="black") + geom_point(aes(y=yhat),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-ma}
\captionof{figure}{Moving average (n=3)}
\label{fig:ma}
\end{center}

\subsection{Weighted Moving average with size n and weighing vector}

n=4
Weighing vector = [0.1 0.2 0.3 0.4] with $\sum_{k=1}^{k=n}w_{k}=1 $

$ \hat{y}_{i} = \sum_{j=1}^{j=n}y_{i-j}*w_{n-j} $

<<label=wma,fig=TRUE,include=FALSE, echo=TRUE>>=
n <- 4
w <- c(0.1, 0.2, 0.3, 0.4)
serie.wma <- serie
serie.wma[nrow(serie.wma)+1,] <- c(nrow(serie.wma)+1,NA)
serie.wma$yhat <- NA
for (i in ((n+1):nrow(serie.wma))) {
  serie.wma$yhat[i] <- 0
  for (j in (1:n)) {
    serie.wma$yhat[i] <- serie.wma$yhat[i] + serie.wma$y[i-j]*w[n-j+1]
  } 
}
ggplot(data=serie.wma,aes(x=x)) + geom_point(aes(y=y),colour="black") + geom_point(aes(y=yhat),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-wma}
\captionof{figure}{Weighted Moving Average (n=4) (w=(0.1 0.2 0.3 0.4))}
\label{fig:wma}
\end{center}

\section{Exponential smoothing}

\subsection{Single exponential smoothing}

You use all former datapoints with a steadily decreasing weight: e.g. 

weighing vector=$\left[  0.9 \quad 0.9^{2} \quad 0.9^{3} \ldots \right]$

However the sum of all the weight should be 1. This is not the case. Poisson, Holt and Roberts found a solution by calculating the expected value in a recursive way:

$ \hat{y}_{i} = \alpha y_{i} + \left( 1-\alpha \right)\hat{y}_{i-1} $

$\alpha$ is the importance that is given to the present observed value. High values of $\alpha$ mean that the influence of former data points fades quickly. $\alpha$ is called the ''smoothing force" or ''smoothing coefficient" but a better name would be ''memory decay rate".

<<label=ses,fig=TRUE,include=FALSE, echo=TRUE>>=
serie.ses <- serie
serie.ses[nrow(serie.ses)+1,] <- c(nrow(serie.ses)+1,NA)
for (k in c(1:2)) {
if (k==1) {
  alpha <- 0.1
  serie.ses$yhat01 <- NA
  serie.ses$yhat01[1] <- serie.ses$y[1]
  for (i in (2:nrow(serie.ses))) {
    serie.ses$yhat01[i] <- alpha*serie.ses$y[i]+(1-alpha)*serie.ses$yhat01[i-1]
    }
  } else {
  alpha <- 0.9
  serie.ses$yhat09 <- NA
  serie.ses$yhat09[1] <- serie.ses$y[1]
  for (i in (2:nrow(serie.ses))) {
    serie.ses$yhat09[i] <- alpha*serie.ses$y[i]+(1-alpha)*serie.ses$yhat09[i-1]
    }
  }
} 
ggplot(data=serie.ses,aes(x=x)) + 
  geom_point(aes(y=y),colour="black") + 
  geom_point(aes(y=yhat01),colour="green") +
  geom_point(aes(y=yhat09),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-ses}
\captionof{figure}{Single Exponential Smoothing (alpha=0.1 green, alpha=0.9 red)}
\label{fig:ses}
\end{center}

Single exponential smoothing using the HoltWinters function give the same results except for the fact that it only gives predictions for (n-1) data points. No result for the last point 

<<>>=
serie.ses$HW <- NA
fit <- HoltWinters(serie$y, alpha=0.9, beta=FALSE, gamma=FALSE)
serie.ses$HW[1:6] <- as.data.frame(fit$fitted)[,1]
@

\subsection{Double Exponential Smoothing}

In the naive method we equated the next expected value to the present observed value:\\

$ \hat{y}_{i}=y_{i-1} $\\

We can improve on that by using the slope (trend) in the datapoints.

If the slope is $ b_{i}=y_{i}-y_{i-1} $ then a better estimate of the next expected value could be\\

$ \hat{y}_{i}=y_{i-1} + b_{i} $

Instead of working with the observed values $y_{i}$ we work with calculated values, called levels, $l_{i}$. Both the levels and the slopes (so ''Double") can be ''exponentialy smoothed":

\begin{align}
l_{i}&=\alpha y_{i}+\left( 1-\alpha  \right) \left( l_{i-1}+b_{i-1}  \right) \\
b_{i}&=\beta \left( l_{i} - l_{i-1} \right) + \left( 1-\beta  \right)b_{i-1} \\
\hat{y}_{i+1}&=l_{i}+b_{i}
\end{align}

<<label=des,fig=TRUE,include=FALSE, echo=TRUE>>=
serie.des <- serie
serie.des[nrow(serie.des)+1,] <- c(nrow(serie.des)+1,NA)
alpha <- 0.9
beta <- 0.9
serie.des$level <- NA
serie.des$slope <- NA
serie.des$yhat <- NA
serie.des$level[1] <- serie.des$y[1]
serie.des$slope[1] <- 0
serie.des$yhat[1] <- serie.des$y[1]
for (i in (2:nrow(serie.des))) {
  if (i==2) {
    new.level <- serie.des$y[i]
    new.slope <- serie.des$y[i]-serie.des$y[i-1]
  } else {
  if (i >= nrow(serie.des)) {
    yvalue <- serie.des$yhat[i-1]
  } else {
    yvalue <- serie.des$y[i]
  }
  last.level <- serie.des$level[i-1]
  last.slope <- serie.des$slope[i-1]
  new.level <- alpha*yvalue + (1 - alpha)*(last.level + last.slope)
  new.slope <- beta*(new.level - last.level) + (1 - beta)*last.slope
  }
  serie.des$yhat[i] <- new.level + new.slope
  serie.des$level[i] <- new.level
  serie.des$slope[i] <- new.slope
}
ggplot(data=serie.des,aes(x=x)) + 
  geom_point(aes(y=y),colour="black") + 
  geom_point(aes(y=yhat),colour="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-des}
\captionof{figure}{Double Exponential Smoothing (alpha=0.9, beta=0.9)}
\label{fig:des}
\end{center}

Same results as Trubetskoy! But when I use the HoltWinters function I get the same results but shifted in time (+1):

<<>>=
serie.des$HW <- NA
fit <- HoltWinters(serie$y, alpha=0.9, beta=0.9, gamma=FALSE)
serie.des$HW[3:7] <- as.data.frame(fit$fitted)[,1]
@

\subsection{Triple Exponential Smoothing}

Terminology:
\begin{itemize}
  \item Season: if a series appears to be repetitive at regular intervals, it is called \emph{seasonal}. \textbf{Seasonality is a precondition for the use of Holt-Winters!}
  \item Season length: L = number of data points after which a new season starts
  \item Seasonal component: an aditional deviation from level + trend. There is a seasonal component for every point in a season.
\end{itemize}

Triple exponential smoothing applies an exponential smoothing to the seasonal components in addition to level and trend. The smoothing is applied across seasons e.g. the seasonal component of the third point into the season will be exponentialy smoothed with the third point of last season, the third component of two seasons ago etc.

\begin{align}
l_{i}&=\alpha \left(y_{i} - s_{i-L}  \right)+\left( 1-\alpha  \right) \left( l_{i-1}+b_{i-1}  \right) \\
b_{i}&=\beta \left( l_{i} - l_{i-1} \right) + \left( 1-\beta  \right)b_{i-1} \\
s_{i}&=\gamma \left( y_{i} - l_{i}  \right) + \left( 1 - \gamma  \right)s_{i-L} \\
\hat{y}_{i+m}&=l_{i} + m b_{i} +s_{i - L + 1 +\left(  m - 1 \right)modL}
\end{align}

Instead of trying to program this myself let's get better acquainted with the HoltWinters function. Trubetskoy uses a new data set with seasonality:

<<label=tes1,fig=TRUE,include=FALSE, echo=TRUE>>=
series.2 <- c(30, 21, 29, 31, 40, 48, 53, 47, 37, 39, 31, 29, 17,
              9, 20, 24, 27, 35, 41, 38, 27, 31, 27, 26, 21, 13, 
              21, 18, 33, 35, 40, 36, 22, 24, 21, 20, 17, 14, 17,
              19, 26, 29, 40, 31, 20, 24, 18, 26, 17, 9, 17, 21,
              28, 32, 46, 33, 23, 28, 22, 27, 18, 8, 17, 21, 31,
              34, 44, 38, 31, 30, 26, 32)
x.2 <- c(1:length(series.2))
serie2 <- data.frame(x=x.2,y=series.2)
ggplot(data=serie2, aes(x=x)) + geom_point(aes(y=y), colour="red") + geom_line(aes(y=y))
@

\begin{center}
\includegraphics[width=0.5\textwidth]{ForeCast-tes1}
\captionof{figure}{Triple Exponential Smoothing: starting data}
\label{fig:tes1}
\end{center}

Now we use HoltWinters. The parameters are:

\begin{itemize}
  \item a time series with observed values (series.2.ts)
  \item alpha ($\alpha$=0.716 - the same value as Trubetskoy)
  \item beta ($\beta$=0.029 - the same value as Trubetskoy)
  \item gamma ($\gamma$=0.993 - the same value as Trubetskoy)
  \item number of points to forecast (24 - the same value as Trubetskoy)
\end{itemize}

<<label=tes2,fig=TRUE,include=FALSE, echo=TRUE>>=
series.2.ts <- ts(series.2, frequency = 12)
fit1 <- HoltWinters(series.2.ts, alpha=0.716, beta=0.029, gamma=0.993)
fc1 <- forecast(fit1,4)
plot(fc1)
param1 <- data.frame(alfa=fc1[["model"]]["alpha"], 
                     beta=fc1[["model"]]["beta"], 
                     gamma=fc1[["model"]]["gamma"])
param1
@

\begin{center}
\includegraphics[width=0.7\textwidth]{ForeCast-tes2}
\captionof{figure}{Triple Exponential Smoothing: forecast}
\label{fig:tes2}
\end{center}

When we let HoltWinters optimize $\alpha$, $\beta$ and $\gamma$ we get about the same values:

<<label=tes3,fig=TRUE,include=FALSE, echo=TRUE>>=
fit2 <- HoltWinters(series.2.ts)
fc2 <- forecast(fit2,4)
plot(fc2)
param2 <- data.frame(alfa=fc2[["model"]]["alpha"], 
                     beta=fc2[["model"]]["beta"], 
                     gamma=fc2[["model"]]["gamma"])
param2
@

\begin{center}
\includegraphics[width=0.7\textwidth]{ForeCast-tes3}
\captionof{figure}{Triple Exponential Smoothing: forecast with HW optimising}
\label{fig:tes3}
\end{center}

\section{Conclusions}

The methods used for forecasting have grown ''naturally" from a naive prediction, to averaging methods, of which HoltWinters is an advanced example. HoltWinters takes into account level, slope and seasonality (periodicity). Level (controlled by $\alpha$) and slope (controlled by $\beta$) are adjustable and therefor more complex than a linear regression through all data points. Seasonality allows for periodicity in the signal and is controlled by $\gamma$. If no values for $\alpha$, $\beta$ and $\gamma$ are given, HoltWinters will find those values that minimize an error function.

It seems that if seasonality is present, it is very important to turn your data vection into a times series objects and give the frequency yourself.

\end{document}